# ğŸ”¬ Multi-LLM Prompt-Optimization Playground

*A Streamlit web-app for **designing, A/B-testing, and analysing prompts** across OpenAI GPT-4o-mini, Anthropic Claude-3 Haiku, and Mistral 7-B models.*

<div align="center">
  <img src="https://raw.githubusercontent.com/your-org/your-repo/main/.github/screenshot.png" width="700" alt="UI screenshot">
</div>

---

## 1. Why this project?

Hiring pipelines often look for engineers who can ship an end-to-end AI productâ€”not just call an API.  
This repo shows exactly that:

* **Product thinking** â€“ a polished UI with five task-oriented pages
* **Engineering rigour** â€“ modular model layer + cost & latency tracking 
* **Research-grade evaluation** â€“ BLEU, ROUGE-L and embedding-cosine metrics  
* **Data discipline** â€“ every run is appended to `history.jsonl` for auditability  

---

## 2. Live demo pages

| Page | What it does | Core file |
|------|--------------|-----------|
| **Single Model Demo** | Send one prompt â†’ see output, token usage, $$ and latency. | `app.py`  |
| **Compare Two Models** | Side-by-side outputs + auto prompt-improvement suggestions. | `app.py` |
| **A/B Test â€“ Weighted-Sum** | Optimise prompts when *no ground truth* exists (qualityÃ—costÃ—latency WSM). | `app.py` |
| **A/B Test â€“ Ground-Truth** | Batch compare summaries against reference text via BLEU / ROUGE / embeddings. | `app.py` |
| **Analytics Dashboard** | Aggregated spend, call counts & latency over time from `history.jsonl`. | `app.py` |

---

## 3. Architecture at a glance

```text
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Streamlit  â”‚  UI / charts / forms
                â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                          .env â†’ API keys
   constants.py â†’  Best-practice copy           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                      â”‚
app.py â”€â”€â”€â–º models.py â”€â–º External LLM APIs (OpenAI, Anthropic, Mistral)
                      â”‚
      metrics.py â”€â”€â–º Quality scores (BLEU, ROUGE-L, cosine) 
       costs.py â”€â”€â”€â–º Cost calculators per vendor 
      logger.py â”€â”€â”€â–º JSONL telemetry store 
